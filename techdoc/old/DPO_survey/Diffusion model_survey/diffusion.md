<!-- 以下latexにする際には$$を解除してコメントアウトを解除して -->
<!-- ### → \subsubsection{}
## → \subsection
# →\section
を変換して投稿すること。 -->

<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>


<!-- \documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}

\begin{document} -->
# 記号一覧
$x \in \mathbb{R}^d に対してx_i でベクトルのi次元目の成分、実数値を表す。$

$D = \{ x^1,x^2,...,x^N\}$で訓練すデータセットをあらわす。

$p(x)$で確率変数$x$に対する確率分布(関数)を表す。



$f(x:\theta)$でエネルギー関数を表す。


# 拡散モデルの概要
拡散モデルの代表例は2022:DALL-E2, Midjourney, Stable Diffusionである。
拡散モデルは従来の生成モデルに比べて優れている点がある。
1. 学習が安定している
2. 難しい生成問題を簡単な部分生成問題に自動分解し、難しいデータ対象も生成可能
3. さまざまな条件付き生成を後付けのプラグインによって実現でき、条件付きの強さを自由に設定できる
4. 生成における対称性、不変性を組み込むことが可能

拡散モデルはデータに確率的なノイズをくわえていき、それを逆向きにた辿ることで、ある確率的なノイズからデータの生成を得るというアイディアに基づいている。

また拡散モデルは潜在変数モデルに基づく生成モデルとみなすことが可能。
つまり、最初のノイズや途中のノイズを加えたデータを潜在変数とみなすことができる。




# 生成モデル

## 生成モデルとはなにか？

生成モデルとは対象ドメインのデータを生成できるモデルのことである。

訓練データ$D = \{ x^1,x^2,...,x^N\}$を考え、これが$p(x)$という確率分布によって、互いに独立にサンプリングされているとする。これは未知の確率分布である。

また生成モデルは$q_\theta(x)$という確率分布をもち、この分布に従ってデータをサンプリングすることを

$$ x \sim q_\theta(x) $$

と書くことにする。$\theta$はパラメータを表し、モデルがニューラルネットによって特徴付けられているならニューラルネットのパラメータを表す。生成モデルの目標は、目標となるデータセットの生成確率分布に近い確率分布$q_\theta(x)$を獲得する関数探しのゲームであると言える。生成モデルの確率分布の差、つまり損失関数に当たる指標はKLダイバージェンスや、最適輸送距離などを利用して測る。

また、今後同時確率分布$p(x,c)$や条件付き確率分布$p(x \mid c)$に従った生成モデルも考えていく。


例えばテキスト$t$に対する画像$x$を生成するような問題を考えると、これは$p(x \mid t)$という条件付き確率に基づく生成問題とみなすことができる。


また一般に何かしらの入力から出力を予測するようなタスクも、入力を条件とした出力の条件付き確率$p(y \mid x)$でモデル化することができる。このようなケースでも確率分布によってモデル化することで、出力に複数の可能性があるようなケースも自然にあつかうことができる。

例えば白黒画像に対してカラー画像を生成するような問題設定において、決定的関数を用いることもできる。しかし実際には白黒画像に対応するカラー画像は多数存在しているはずで、それらすべてを決定的な関数で扱うことは難しい。このように確率分布関数を用いてモデル化することは多数の利点が考えられる。


## エネルギーベースモデルと分配関数

一般にデータ$x \in X$の生成モデルの確率分布$q_\theta(x)$は次のような形で表される。

$$
q_\theta(x) = \gamma_\theta(x) / Z(\theta) 
$$

$$
Z(\theta) = \int_{x \in X}  \gamma_\theta(x) dx
$$

ここで$\gamma_\theta (x) \gt 0$を非正規化密度関数と呼ぶ。また$Z(\theta)$のことを分配関数といい、$q_\theta(x)$の値が$[0,1]$区間に収まるように調整する役割を果たしている、すべてのデータ$x \in X$に渡る積分をとっている。

一般的に分配関数はすべてのデータに対する積分を実行する必要があり、計算が困難である。
逆に言えば分配関数がデータ空間に関するすべての情報を持っているという言い方もできる。

また統計力学から着想をえて、非正規化確率密度関数をエネルギー関数$f(x:\theta):\mathbb R^d \rightarrow \mathbb R$でパラメトライズして$\gamma_\theta(x)=\exp(-f(x:\theta))$と表した確率モデルを「エネルギーベースモデル」という。エネルギー関数には数学的な制約はなく、任意の実数値を取りうる。

$$
q_\theta(x) = \exp(-f_\theta(x)) / Z(\theta) 
$$

$$
Z(\theta) = \int_{x \in X}  \exp(-f_\theta(x)) dx
$$

この場合、エネルギー関数が小さい値をとれば、そのデータ$x$は出現しにくいことを表している。
分配関数からデータ全体に関する様々な統計量を抽出することが(すくなくとも原理的には)可能である。

入力データが高次元であったり、連続変数(つまり分配関数が純粋に積分をしないといけない)場合、基本的にエネルギー関数が特別な性質をもっていたりしない限り、この分配関数の値や勾配を求めることは困難である。


またエネルギーベースモデルのもう一つの特徴として二つのエネルギー関数$f_1(x)$, $f_2(x)$が与えられた時に、それらから得られる確率分布が$q_1(x)$, $q_2(x)$であるとする。

この時に二つのエネルギーの和$f_1+f_2$をエネルギーとしてもつような確率分布$q(x)$は

$$
q(x) \propto \exp(-f_1(x) - f_2(x)) \\
= q_1(x) q_2(x)
$$


これはエネルギーを加えることで、二つの確率分布の積に比例する分布が得られることを表している。


## 学習手法

ここでは生成モデルの学習手法を説明する。生成モデルの学習手法は大きく二つある。

#### 尤度ベースモデル

与えられたデータ$x$の生成確率$q_\theta(x)$を尤度という。尤度を最大化するパラメータを推定する学習手法を最尤推定法という。

訓練データセットが$D = \{x^1,x^2,...,x^N \}$で与えられているとき、各データが独立にサンプリングされているので、データセットに対する尤度は以下の形で定義される。

$$
q_\theta(D) = \prod_t q_\theta(x^t)
$$

微分などを実行して最適化問題として扱いやすい対数尤度を以下で定義する。

$$
L(\theta) = \log q_\theta(D) = \sum_t \log q_\theta(x^t)
$$

変分自己符号化器(VAE)や自己回帰モデル、ああいに正規化フローモデル、エネルギーベースモデルといった生成モデルが尤度ベースモデルと言える。このような最尤推定法のゴールは尤度関数、つまりこの場合対数尤度関数$L(\theta)$を最大化することである。

エネルギーベースモデルの場合に対数尤度関数をエネルギー関数を含んだ形に書き換えると以下のようになる。

$$
\begin{align*}
L(\theta) &= \frac{1}{N} \sum_{t=1}^N \log q_\theta(x^t) \\
&= - \frac{1}{N} \sum_{t=1}^N \left[ f_\theta(x^t)\right] -\log Z(\theta) \\
&= - \frac{1}{N} \sum_{t=1}^N \left[ f_\theta(x^t)\right] -\log \int \\
\end{align*}
$$


#### 暗黙的生成モデル



















<!-- \end{document} -->